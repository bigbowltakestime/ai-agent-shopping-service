#!/usr/bin/env python3
"""
ì›¹ í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì˜¬ë¦¬ë¸Œì˜ ìŠ¤í‚¨ì¼€ì–´ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ê³  CSV ë° SQLiteì— ì €ì¥
"""

import sys
import os
import json
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from core.spider import WebSpider
from core.selenium_extractor import SeleniumProductExtractor

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì˜¬ë¦¬ë¸Œì˜ ìŠ¤í‚¨ì¼€ì–´ ìƒí’ˆ í¬ë¡¤ëŸ¬')
    parser.add_argument('--max-pages', type=int, default=1,
                       help='í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 1)')
    parser.add_argument('--output-dir', type=str, default='output',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: output)')
    parser.add_argument('--no-detailed', action='store_true',
                       help='ìƒì„¸ ì •ë³´ ì¶”ì¶œ ë¹„í™œì„±í™” (ê¸°ë³¸ì ìœ¼ë¡œ ì¼œì ¸ìˆìŒ)')
    parser.add_argument('--max-reviews', type=int, default=10,
                       help='ìƒí’ˆë‹¹ ìµœëŒ€ ë¦¬ë·° ìˆ˜ (ê¸°ë³¸ê°’: 10)')

    args = parser.parse_args()

    # ìƒì„¸ ì •ë³´ ì¶”ì¶œì´ ê¸°ë³¸ì ìœ¼ë¡œ ì¼œì ¸ìˆìœ¼ë©°, --no-detailed í”Œë˜ê·¸ë¡œ ë„ê¸° ê°€ëŠ¥
    args.detailed = not args.no_detailed

    print("ğŸ› ì˜¬ë¦¬ë¸Œì˜ ìŠ¤í‚¨ì¼€ì–´ ìƒí’ˆ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print(f"ğŸ“„ í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜: {args.max_pages}")
    print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: web_crawler/{args.output_dir}")
    print(f"ğŸ” ìƒì„¸ ì •ë³´ ì¶”ì¶œ: {'ì¼œì§' if args.detailed else 'êº¼ì§'}")
    print("-" * 50)

    try:
        # WebSpider ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        spider = WebSpider()

        # í¬ë¡¤ë§ ì‹¤í–‰
        products = spider.crawl_products(max_pages=args.max_pages)

        # ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ì„ íƒì )
        if args.detailed and products:
            print(f"\nğŸ” ìƒì„¸ ì •ë³´ (ì„±ë¶„, ë¦¬ë·°) ì¶”ì¶œ ì¤‘... (Selenium)")
            print(f"   - ìƒí’ˆë‹¹ ìµœëŒ€ ë¦¬ë·° ìˆ˜: {args.max_reviews}")


            try:
                with SeleniumProductExtractor(headless=True) as extractor:
                    # ëª¨ë“  ìƒí’ˆì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                    enriched_products = extractor.batch_extract_details(
                        products,
                        max_reviews=args.max_reviews
                    )
                    # ìƒì„¸ ì •ë³´ê°€ ì¶”ê°€ëœ ìƒí’ˆìœ¼ë¡œ êµì²´
                    products = enriched_products
                print("âœ… ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ!")
            except Exception as e:
                print(f"âŒ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                print("ğŸ“ ê¸°ë³¸ ì •ë³´ë§Œìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

        # ê²°ê³¼ ì €ì¥
        spider.save_to_csv(products)
        spider.save_to_sqlite(products)

        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")

        if products:
            print("\nğŸ“Š ì €ì¥ëœ íŒŒì¼:")
            print(f"   - CSV: web_crawler/{args.output_dir}/products.csv")
            print(f"   - SQLite: web_crawler/{args.output_dir}/products.db")

            if args.detailed:
                print("   âœ… ì„±ë¶„ ì •ë³´ í¬í•¨ë¨")
                print(f"   âœ… ìƒí’ˆë‹¹ ìµœëŒ€ {args.max_reviews}ê°œ ë¦¬ë·° í¬í•¨ë¨")

        print("\nğŸ‰ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ!")

    except KeyboardInterrupt:
        print("\nâ¹ï¸  ì‚¬ìš©ìê°€ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
